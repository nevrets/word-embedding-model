{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mxnet\n",
    "# !pip install gluonnlp pandas tqdm\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import mxnet.gluon.data\n",
    "\n",
    "# ★ Hugging Face를 통한 모델 및 토크나이저 Import\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from kobert import get_tokenizer\n",
    "from kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/analysis-2/tipa_analysis/.cache/kobert_v1.zip\n",
      "using cached model. /home/analysis-2/tipa_analysis/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "## GPU\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/analysis-2/tipa_analysis/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# dataset_train = nlp.data.TSVDataset(\".cache/ratings_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset = pq.read_table('tipa_text_tokens_20220328.parquet').to_pandas()\n",
    "dataset['label'] = 0\n",
    "dataset = dataset[['sbjt_id', 'main_str', 'label']].loc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_csv('out_sample.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nlp.data.TSVDataset('out_sample.txt', field_indices=[1, 2], num_discard_samples=1)\n",
    "# dataset_ = pd.read_csv('out_sample.txt', sep='\\t')\n",
    "# dataset_ = dataset_.drop(['label'], axis=1)\n",
    "\n",
    "# field_indices=[1, 2] : 데이터가 1번 index, 레이블이 2번 index\n",
    "# num_discard_samples=1 : 상단 1개의 row를 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = []\n",
    "# for doc in dataset_['main_str']:\n",
    "#     documents.append([doc])\n",
    "\n",
    "# ds = mxnet.gluon.data.SimpleDataset(documents)\n",
    "# # ds = mxnet.gluon.data.SimpleDataset([['나 보기가 역겨워', '김소월']])\n",
    "# tok = tokenizer.tokenize\n",
    "# trans = nlp.data.BERTSentenceTransform(tok, max_seq_length=10, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "    \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/analysis-2/tipa_analysis/kobert_sentence.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.7.0.19/home/analysis-2/tipa_analysis/kobert_sentence.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m BERTDataset(dataset, \u001b[39m1\u001b[39;49m, tok, max_len, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/analysis-2/tipa_analysis/kobert_sentence.ipynb Cell 11'\u001b[0m in \u001b[0;36mBERTDataset.__init__\u001b[0;34m(self, dataset, sent_idx, bert_tokenizer, max_len, pad, pair)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.7.0.19/home/analysis-2/tipa_analysis/kobert_sentence.ipynb#ch0000010vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, dataset, sent_idx, bert_tokenizer, max_len, pad, pair):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.7.0.19/home/analysis-2/tipa_analysis/kobert_sentence.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m     transform \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mBERTSentenceTransform(bert_tokenizer, max_seq_length\u001b[39m=\u001b[39;49mmax_len, pad\u001b[39m=\u001b[39;49mpad, pair\u001b[39m=\u001b[39;49mpair)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.7.0.19/home/analysis-2/tipa_analysis/kobert_sentence.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences \u001b[39m=\u001b[39m [transform([i[sent_idx]]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dataset]\n",
      "File \u001b[0;32m~/anaconda3/envs/edap/lib/python3.9/site-packages/gluonnlp/data/transforms.py:941\u001b[0m, in \u001b[0;36mBERTSentenceTransform.__init__\u001b[0;34m(self, tokenizer, max_seq_length, vocab, pad, pair)\u001b[0m\n\u001b[1;32m    <a href='file:///home/analysis-2/anaconda3/envs/edap/lib/python3.9/site-packages/gluonnlp/data/transforms.py?line=938'>939</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pad \u001b[39m=\u001b[39m pad\n\u001b[1;32m    <a href='file:///home/analysis-2/anaconda3/envs/edap/lib/python3.9/site-packages/gluonnlp/data/transforms.py?line=939'>940</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pair \u001b[39m=\u001b[39m pair\n\u001b[0;32m--> <a href='file:///home/analysis-2/anaconda3/envs/edap/lib/python3.9/site-packages/gluonnlp/data/transforms.py?line=940'>941</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mvocab \u001b[39mif\u001b[39;00m vocab \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m vocab\n\u001b[1;32m    <a href='file:///home/analysis-2/anaconda3/envs/edap/lib/python3.9/site-packages/gluonnlp/data/transforms.py?line=941'>942</a>\u001b[0m \u001b[39m# RoBERTa does not register CLS token and SEP token\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/analysis-2/anaconda3/envs/edap/lib/python3.9/site-packages/gluonnlp/data/transforms.py?line=942'>943</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab, \u001b[39m'\u001b[39m\u001b[39mcls_token\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "data = BERTDataset(dataset, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "    optimizer.zero_grad()\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_attention_mask(self, token_ids, valid_length):\n",
    "    attention_mask = torch.zeros_like(token_ids)\n",
    "    for i, v in enumerate(valid_length):\n",
    "        attention_mask[i][:v] = 1\n",
    "    return attention_mask.float()\n",
    "\n",
    "# def forward(self, token_ids, valid_length, segment_ids):\n",
    "attention_mask = gen_attention_mask(token_ids, valid_length)\n",
    "    \n",
    "_, pooler = bertmodel(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ★\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, bert_tokenizer, vocab, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ★\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "data_train = BERTDataset(dataset, 0, tok, vocab, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gluonnlp.data.dataset.TSVDataset object at 0x7fe9a46a96a0>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, token_ids, valid_length, segment_ids):\n",
    "    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "    \n",
    "    _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "    if self.dr_rate:\n",
    "        out = self.dropout(pooler)\n",
    "    return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KR-SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('KR-SBERT/KR-SBERT-V40K-klueNLI-augSTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pq.read_table('tipa_text_tokens_20220328.parquet').to_pandas()\n",
    "dataset = dataset.loc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for doc in dataset['main_str']:\n",
    "    documents.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999999 , 0.49660236, 0.52995765, ..., 0.4263104 , 0.48406532,\n",
       "        0.09445414],\n",
       "       [0.49660236, 0.99999976, 0.4284887 , ..., 0.31369466, 0.43486148,\n",
       "        0.1000298 ],\n",
       "       [0.52995765, 0.4284887 , 0.99999994, ..., 0.41751313, 0.5828234 ,\n",
       "        0.13223591],\n",
       "       ...,\n",
       "       [0.4263104 , 0.31369466, 0.41751313, ..., 1.0000005 , 0.49146536,\n",
       "        0.16618317],\n",
       "       [0.48406532, 0.43486148, 0.5828234 , ..., 0.49146536, 0.99999994,\n",
       "        0.23146369],\n",
       "       [0.09445414, 0.1000298 , 0.13223591, ..., 0.16618317, 0.23146369,\n",
       "        0.9999998 ]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = documents\n",
    "vectors = model.encode(sentences)\n",
    "similarities = cosine_similarity(vectors)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 101)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('KR-SBERT/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "\n",
    "corpus = documents\n",
    "\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: 인공지능\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "제품 개발 (Score: 0.2793)\n",
      "유체점도 측정장비 개발 (Score: 0.2252)\n",
      "차원 입체형상 당사에서 활용하고자 하는 Rapid Prototyping System이하 RP이란 쾌속조형기또는 rapid prototyping system이라고 하는데 설계 단계에 있는 차원 모델을 실용적이고 현실적인 모형이나 시제품Prototype으로 다른 중간 과정 없이 빠르게 생성하는 새로운 기술을 말합니다 CAD는 차원 데이터를 보유하고 있지만 그 표시는 모니터 화면 또는 그 Hard Copy이므로 출력할 때는 차원으로 밖에 되지 않습니다 반면 RP는 설계된 차원 CAD데이터를 차원 단면 데이터로 변환한 후 여러 가지 방법RP공정을 적용하여 순차적으로 적층해 감으로써 CAD데이터와 같은 차원 입체 형상을 제작하여 원하는 시제품을 얻어 낼수 있습니다 CAD상에서 설계한 데이터를 이용하여 바로 실물MOCKUP로 제작하고 만들어진 실물로부터 설계오류를 확인하여 궁극적으로 설계 완성도를 향상시키고 상품개발기간을 줄이고자 하는 목적으로 많이 활용되고 있다RP공정을 통해 얻은 모형은 당사의 주 원재료가 됩니다 오늘날 개방화 국제화 시대를 맞이하여 고객들의 의식수준이 향상되면서 다양한 제품에 대한 감각도 날로 향상되고 개성있는 제품을 선호하는 경향이 점점더 짙어지고 있습니다 따라서 각 기업들은 유수한 세계 기업들과 경쟁하기 위해서 원가절감 및 품질 혁신이 절실이 요구되고 다양한 고객들의 요구에 부합하기 위해서 다양한 제품을 생산해야 하는 실정입니다 또한 고객의 디자인에 대한 욕구를 만족 시키기 위해서 개성이 중시된 디자인 요소가 가미된 제품을 생산해야 됩니다또한 국내외 시장에서 경쟁이 치열해지면서 제품의 개발기간과 시작기간의 단축이 요청되고 있으며 이에 따른 시장 도입시기를 앞당기려는 노력이 경주되고 있으며 제품개발에 관련된 납기 단축이 중요한 문제로 대두되었습니다 특히 당사와 같이 미술작품을 제작하는 경우 모형의 제작이 기본이 되고 작품의 완성도에 직접적인 영향을 주기때문에 모형을 얼마나 신속하고 정확하게 제작되느냐가 가장 중요한 문제입니다 RP 즉 Rapid Prototyping은 컴퓨터에 저장된 차원 형상모델의 기하하적 자료로부터 그 물리적인 모형형상을 신속하게 조형해 내는것으로 그 이전에 존재하였던 그어떤 가공방식과도 비교할수 없는 빠른시간내에통상 시간 물리적인 모형으로 재현해 낸다는것이 가장 큰 장점입니다 CAD상에서 설계한 데이터를 이용하여 바로 실물MOCKUP로 제작하고 만들어진 실물로부터 설계오류를 확인하여 궁극적으로 설계 완성도를 향상시키고 상품개발기간을 줄이고자 하는 목적으로 많이 활용되고 있습니다이로 인한 시간단축으로 가장크게 기대할수 있는 점이 비용절감입니다 RP공정을 통해 얻은 모형은 당사의 원재료가 되어 향후 작품으로 완성되어 수출되거나 국내 미술관에 전시될 예정입니다 (Score: 0.2211)\n",
      "바이오 연구개발 및 시생산 설비 사용 (Score: 0.2190)\n",
      "산업 디자인제품 샘플제작 산업디자인제품개발및 로봇연구개발을 위하여 정확한 치수와 형태가공이 용이한 D프린터를 이용하여 제품의 샘플링작업을 할 계획이며 디자인면에서 다양한 제품을 손 쉽게 MOCKUP으로 볼수있으므로 제품을 확인및 보완 가공이 용이할 것으로 기대하고 있습니다로봇연구시에 정확한 치수와 다양한 부속품을 가공하여하는데 쾌속 조형기로 작업시 많은 작업시간단축과 보완 가공및 정확한 수치와 형태의 제품을 얻을수 있을것으로 기대하고 있습니다 보다 정확한 치수와 형태로인하여 기존 CNC머신으로 하던 작업들에 불가능한 부분을 커버할 수 있을거 같으며 시간 절약및 빠른시간안에 원하는 제품의 샘플을 얻을 수 잇는점이 장비사용의 장점입니다로봇연구개발시에 많고 다양한 부속품과 정확한 치수를 필요로 하는 부품들이 많기 때문에 보다 정교하고 빠른 장비를 사용하여 연구개발 시간을 절약하고 보다 완성도 높은 제품을 개발하기위해 쾌조속형기등의 장비를 이용하여야할 것 같습니다 쾌속조형기등 D프린터를 이용하면 우선 첫째로 많은 시간을 줄일 수 있을것으로 기대되며 제품의 디자인시 다양한 디자인을 손쉽세 제품샘플링 할수 있어서 디자인측면에서는 보다 더 좋은 디자인을 쉽게 선택하는데 도움을 줄 수 있을거 같습니다작업시에도 제품의 단점보완등이 용이할 것으로 생각되고 작은 부속품및 제품의 정확한 형태와 치수가 제품 샘플링시에 바로 확인가능하여 보다 적극적인 작업효과를 기대하고 있습니다 (Score: 0.1958)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: 인공지능과 자동차\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "자동차 네비게이션 장착안전성확보방안 자동차 내장 악세사리를 만드는 기업입니다 네비게이션 장착이 차량내에서 보다 편리하고 안전한 방법으로 장착이 가능하도록 연구하고 있습니다 매스컴에 가끔 보도되는바와 같이 현재 일반인들이 대게 장착하고 다니는 실내 전면유리창 흡착방식의 네비게이션은 후방추돌사고시 동승자와 운전자의 안전을 좀먹는 무기로 바뀌게 되는데 보다 근본적이고 안전한 방식으로 차량내에 설치를 할수있도록 각 차량에 맞는 마감재를 만들어 안전상의 문제 대낮의 햇빛이 강하면 화면이 또렷이 보이지않는 문제 등을 개선해보려고 합니다 장비를 활용하여 역설계하는데 도움을 받아 신제품 설계에 유용하게 사용코저합니다 고가의 장비라서 우리회사에 구매가 당장은 어렵습니다 도움을 받고자 원합니다 d 스캔 장비가 있어야 순정차량의 데쉬보드 상단의 형상을 읽어낼수 있으며 이 데이터가 있어야 기본적으로 차량내의 형상과 초기설계를 할수 잇습니다 모든 차량이 신차종으로 모델변경시 기존의 데쉬보드를 그대로 사용하는 예가 없고 거의 모든차량은 페이스 리프트 되는 경우는 제외 실내 디자인이 완전히바뀌어서 나오기때문에 매번 신차종출시때마다 신제품 개발을 위해서 스캔장비를 이용하여 차량 실내를 측정하여야 합니다 지금까지는 개인업체에 의뢰해서 고가의 장비사용료와 역설계비용을 지불해왔습니다 국내시장여건상 굉장히 고가의 비용을 지불해야 하며 개인업체의 스케줄에 의해 때로는 타이밍을 놓쳐 제품 적기개발 생산 시기를 놓치고 경쟁사와의 싸움에서 밀리기도 합니다 매번 신차종 출시후 신제품 출시를 위해 제품개발시작단계에서 많은 어려움이 비용적으로나 시간적으로나 있었습니다 장비를 활용할수있게 된다면 제품개발시기를 굉장히 앞당기고 적기에 제품생산을 할수있을것으로 기대됩니다 (Score: 0.2838)\n",
      "지그제작 연구장비 공동이용 지원사업을 통하여 미보유 장비를 공인기관에 의뢰하여 연구개발에 필요한 고정도의 시험평가 장비와 우수한 연구 인력을 활용 하여 원가절감 및 고객의 신뢰성 확보하고 기술의 선진화 및 지속발전에 원동력이 되도록 활용 하고자 합니다 고도의 기술 개발과 첨단화 되어가는 자동차 기술 에 발위해서는 정밀 기술이 요구됨니다고품질 제품을 개발및 제작하기 위하여 제품의 품질을 검증하기 위해 차원측정을 하려고함 기술개발 을 통한 고속화 경량화 친환경 녹색기술 하이브리드 자동차 전기 자동차량 의 개발 고도의 기술개발 과 첨단화 되어가는 세계의 자동차 기술 금형 제작 및 정밀 부품의 개발 고정밀도의 측정장비 활용으로 신뢰성 확보연구기관 의 장비를 사용함으로서 초 고가의 장비를 직접 투자 하지않고 안정된 연구개발 환경을 구축하고 기업의 경쟁력 향상 과 글로벌 화에 기여하고 나아가 고보가 가치 핵심 금형기술 개발 선진화 에 원동력이 되고자 노력 자동차 및 산업용 오일레벨 게이지셋트를제작하는업체로 전직원이 근면과 경영혁신 기술혁신을 바탕으로한 근무환경의 창출을 통한 제품생산의 선도 기술력 강화 및 차별화된 요구로 부합되는 품질 경쟁력 확보 및 신뢰성 확보기술경쟁력 향상과 제품의 생산성 향상 고부가 가치 증대로 경쟁력 강화 정밀한 금형과제품을 만들어 고객의 신뢰성 향상 차원 측정과 열싸이클시험을 통한 품질신뢰를 향상 시킬수 있을것으로기대함니다 (Score: 0.2736)\n",
      "산업 디자인제품 샘플제작 산업디자인제품개발및 로봇연구개발을 위하여 정확한 치수와 형태가공이 용이한 D프린터를 이용하여 제품의 샘플링작업을 할 계획이며 디자인면에서 다양한 제품을 손 쉽게 MOCKUP으로 볼수있으므로 제품을 확인및 보완 가공이 용이할 것으로 기대하고 있습니다로봇연구시에 정확한 치수와 다양한 부속품을 가공하여하는데 쾌속 조형기로 작업시 많은 작업시간단축과 보완 가공및 정확한 수치와 형태의 제품을 얻을수 있을것으로 기대하고 있습니다 보다 정확한 치수와 형태로인하여 기존 CNC머신으로 하던 작업들에 불가능한 부분을 커버할 수 있을거 같으며 시간 절약및 빠른시간안에 원하는 제품의 샘플을 얻을 수 잇는점이 장비사용의 장점입니다로봇연구개발시에 많고 다양한 부속품과 정확한 치수를 필요로 하는 부품들이 많기 때문에 보다 정교하고 빠른 장비를 사용하여 연구개발 시간을 절약하고 보다 완성도 높은 제품을 개발하기위해 쾌조속형기등의 장비를 이용하여야할 것 같습니다 쾌속조형기등 D프린터를 이용하면 우선 첫째로 많은 시간을 줄일 수 있을것으로 기대되며 제품의 디자인시 다양한 디자인을 손쉽세 제품샘플링 할수 있어서 디자인측면에서는 보다 더 좋은 디자인을 쉽게 선택하는데 도움을 줄 수 있을거 같습니다작업시에도 제품의 단점보완등이 용이할 것으로 생각되고 작은 부속품및 제품의 정확한 형태와 치수가 제품 샘플링시에 바로 확인가능하여 보다 적극적인 작업효과를 기대하고 있습니다 (Score: 0.2599)\n",
      "자동차브레이크 패드 브레이크슈 (Score: 0.2470)\n",
      "역설계 및 시제품제작 삼차원 측정기D Scanner를 활용한 제품 역설계 및 품질검사 자동차 부품 기술개발 관련 동종업계 부품 역설계 및 비교분석 조선 항공 기계부품 D 데이터를 활용한 구조해석 및 시뮬레이션 원자력 발전소 설비 노후제품 개선 품질향상 및 국산화 쾌속 조형기 Rapid Prototyping System를 활용한 프로토 타입 제작 기술 홍보 및 발표용으로 활용 제품 설계 후 시제품 제작하여 장착 제품의 벤치마킹 및 신기술 개발에 활용 가능한 역설계reverse engineering데이터는 삼차원 측정기에서 획득한 형상데이터를 필요로 함 삼차원 측정장비는 레이져와 광학식 등의 종류가 있으며 소형제품을 측정 할 수 있는 장비와 대형제품을 측정 할 수 있는 장비로도 나뉜다 각각의 제품에 맞는 장비를 활용하여 정확한 데이터를 얻는 것이 매우 중요하다 요즘 같은 디자인제품 시대의 자유곡면 형상 제품들은 수작업으로 형상치수를 얻어내는 것은 불가능하기 때문에 차원 좌표를 생성하여 형상을 만들어내는 측정기의 필요성이 절실하다 수입제품 국산품으로 대체 수입제품의 가격 상승과 원하는 시기에 수급이 어려운 상황에서 제품을 국산화 함으로써 비용절감과 원활한 제품공급을 기대 연구기간 단축 D CAD를 활용 해 설계 오류와 시간을 단축 개발비 절감 충분한 벤치마킹을 통해 초기 개발비를 절감 품질 향상 제품 생산 전 충분한 비교분석 및 D CAD를 통한 해석 시뮬레이션을 시행하여 보다 나은 제품생산 신제품 개발 및 특허 기존 제품의 개선에 그치지 않고 신기술을 활용한 제품을 개발 (Score: 0.2422)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: 밸브의 고장 유무를 판단하는 인공지능\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "자동차브레이크 패드 브레이크슈 (Score: 0.5284)\n",
      "정유 및 석유화학 플랜트 설비의 파손원인 분석 및 안정성 진단 (Score: 0.4527)\n",
      "태양광설비 및 신차부품 신뢰성검증 (Score: 0.4481)\n",
      "신규 개발차종의 신뢰성 시험 신규 개발 차종의 조수석 에어백을 개발함에 있어서 플라스틱 부품인 도어를 고객의 요구 조건에 맞추어 설계를 진행하고 제품을 개발하여야 합니다 고객 요구에 맞는 제품을 개발하고 난 뒤 에어백 도어는 실제 차에 장착된 상태에서 년 이상의 내구성을 가져야 하며 어떠한 환경 조건에서도 원래의 기능을 보유하고 있어야 때문에 신뢰성 시험을 실시하여야 합니다 이러한 신뢰성 시험을 마친 도어 부품에 대하여 정적 전개 시험을 실시하여 문제가 없어야 정상적인 부품의 양산이 이루어지게 됩니다 이러한 신뢰성 시험에는 여러가지 항목의 환경 시험들이 있으며 복합 염수 분무 시험 내광성 시험 태양광 모의 시험 에미션 시험 등이 있습니다 고객의 요구 조건에 맞는 에어백 도어 부품을 개발하고 나면 신뢰성 시험을 수행하여야 합니다 신뢰성 시험 중 내구성을 확인하기 위한 환경시험으로 여러가지 항목의 환경 시험을 거쳐야 합니다 일부 시험들은 고객사 및 당사의 시험 장비를 이용하여 시험을 수행하고 있지만 복합 염수 분무 시험 내광성 시험 태양광 모의 시험 에미션 시험 등은 장비를 보유하고 있지 않습니다 따라서 이러한 각종 시험 장비를 보유하고 있는 시험 기관에 의뢰하여 환경 시험을 수행하여야만 신뢰성 시험을 종결하게 되며 시험 결과에 대하여 고객의 승인이 있어야만 제품 개발이 완료되게 됩니다 각종 환경 시험 장비를 활용함에 따라 고객이 요구하는 신뢰성 시험을 완료할 수 있으며 시험 결과를 제출하여 고객의 승인을 득하여야만 양산 작업한 에어백 도어 제품을 고객에게 납품할 수 있습니다 고객의 승인을 득하여 양산 납품을 하게되면 두가지 에어백 도어 타입에 대하여 연간 약 억원 이상의 신규 매출이 발생하게 되며 명 이상의 신규 고용 창출이 발생하게 됩니다 또한 자체 개발 능력을 한층 더 높일 수 있어 향후 신규 프로젝트 수주에 도움이 됩니다 (Score: 0.4336)\n",
      "원자로 노내열전대용 상온보상기함 개발 개발제품명칭 노내열전대용 상온보상기함 목 적 Reference Junction Temperature의 온도를 정밀 정확하게 측정하는 Thermocouple Reference Junction Box의 품질등급 Q CLASS의 국산화 개발 용 도 원자로 로심온도 측정에서 Reference Junction Temperature의 온도를 주의 온도변화에 영향을 받지 않고 일정한 온도로 유지시켜 정밀정확하게 온도를 측정하게 하는 Thermocouple Reference Junction Box 성능 및 특성 Junction Accuracy Class 열전대 보상도선을 사용할 경우 Temperature StabilityLong term and Uniformity 온도오차 Set point의 최대 이내 Chanel 간 오차 이내 평균 제어온도로부터 다른 접점의 최대편차 목 적 Reference Junction Temperature의 온도를 정밀 정확하게 측정하는 Thermocouple Reference Junction Box의 품질등급 Q CLASS의 국산화 개발 필요성 발전소 건설 당시부터 최근까지 약 년 이상 사용하여 경년변열화 및 소손으로 온도측정 오차의 증가 등이 발생하여 시급한 국내 기술개발 필요 경쟁성 및 시장성 경쟁성 경쟁제품 대비 동등 이상의 성능 및 이하의 가격 확보를 통한 시장 경쟁력 확보 시장성 영광 호 고리 호기에 판매가능 약 억원SET 천원SET 정도의 매출신장 기대 경쟁성 및 시장성 경쟁성 경쟁제품 대비 동등 이상의 성능 및 이하의 가격 확보를 통한 시장 경쟁력 확보 시장성 영광 호 고리 호기에 판매가능 약 억원SET 천원SET 정도의 매출신장 기대 사용자 측면 고장 발생 시 고장처리에 대한 시간 및 비용 감소 제품 유지보수 및 예비품 확보 용이 세메오스 측면 약 억원의 매출 증대 기대 원전 계측 핵심 기술 국산화를 통한 회사 이미지 제고 (Score: 0.4169)\n"
     ]
    }
   ],
   "source": [
    "queries = ['인공지능',\n",
    "           '인공지능과 자동차',\n",
    "           '밸브의 고장 유무를 판단하는 인공지능']\n",
    "\n",
    "top_k = 5\n",
    "for query in queries:\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    cos_scores = cos_scores.cpu()\n",
    "\n",
    "    top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for idx in top_results[0:top_k]:\n",
    "        print(corpus[idx].strip(), \"(Score: %.4f)\" % (cos_scores[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7e65668cf5155ec43a5ae3b61ed7b79485c295e63c44f8506c88844c5224d44"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('edap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
